# 활성 컨텍스트

## 현재 작업 초점
- 프론트엔드 대시보드(DashboardWidgetGrid.vue) UI/UX 대폭 개선
  - 위젯/검색결과 선택 시 배경색 변경
  - 검색 중 로딩 애니메이션 및 입력/버튼 비활성화
  - 검색 패널 동작(토글/항상표시/검색전 숨김 등) 단계적 개선
  - Lottie 기반 빈 슬롯 애니메이션 및 커스텀 스타일
  - 최근 검색어 태그(로컬스토리지, 클릭 시 재검색)
  - 검색창/태그 시각적 분리, 전체 레이아웃/여백 개선
- 에너지 위젯 9종(동적 import, /widgets/energy/) 개발 및 연결
- vue-toastification → alert로 전환, HMR/동적 import 오류 해결

## 최근 변경사항
- DashboardWidgetGrid.vue UI/UX 전면 개선
- 에너지 위젯 9종 개발 및 동적 import 적용
- 검색 패널, 최근 검색어, Lottie 애니메이션 등 구현
- vue-toastification 제거 및 alert 대체
- HMR/동적 import 오류 해결

## 다음 단계
- 위젯별 데이터 시뮬레이션/애니메이션/새로고침 등 세부 기능 강화
- 대시보드 전체 스타일/반응형 개선
- memory-bank 내 productContext, systemPatterns 등 구체화

## 활성화된 결정사항과 고려사항
- 모든 규칙/가이드라인은 실제 코드와 일치하도록 유지
- memory-bank 문서는 프로젝트 변경 시 즉시 반영
- 코드 품질 및 보안, 테스트 강화 우선
- 위젯 import 경로/동적 import 관련 HMR 문제 해결

## 현재 작업 중인 기능

1. 프론트엔드 대시보드/위젯 UI/UX 개선 및 기능 확장
2. 위젯별 데이터 시뮬레이션/애니메이션/새로고침 등 세부 기능 강화

## 현재 고려사항

1. 코드 품질
   - 일관된 코딩 스타일 유지
   - 테스트 커버리지 확보
   - 문서화 강화

2. 보안
   - 환경 변수 관리
   - API 보안
   - 데이터 보호

3. 성능
   - 캐싱 전략
   - 데이터베이스 최적화
   - 응답 시간 개선

## 미해결 질문

1. 위젯별 실데이터 연동 및 성능 최적화 방안
2. 대시보드 반응형/접근성 강화 방안

## 참고사항
- 이 문서는 현재 작업 상황을 반영하도록 자주 업데이트하세요
- 현재 상황과 결정사항에 집중하세요

## 랭체인 및 벡터DB 구조 Q&A (2024-06-05)

### 1. 테이블형 vector(DB)와 cromadb형 vector를 나누는 게 좋은가?
- 업무/서비스 로직(intents, patterns, responses, routes, widget 등)은 DB 기반 벡터로 관리
- 사내 문서 등 비정형 데이터는 ChromaDB 등 별도 벡터DB로 관리
- **정형(업무) vs 비정형(문서) 분리 유지가 유지보수, 확장성, 보안에 유리**

### 2. 랭체인 기반 ChromaDB 벡터 생성 시 카테고리 등 메타데이터로 LLM에 특정 카테고리 데이터만 전달 가능?
- LangChain VectorStore(Chroma 등)는 메타데이터 필드 지원
- 문서 임베딩 시 카테고리 등 메타데이터 저장, 검색/LLM 연동 시 필터링 가능
- **카테고리 등으로 원하는 문서만 LLM에 전달하는 것이 완벽하게 가능**

### 3. 특정 카테고리는 유사검색어 검색할 때만 참고할 수 있는지?
- 검색 시 메타데이터 필터로 특정 카테고리만 대상으로 유사도 검색 가능
- 쿼리 분석 → 카테고리 추출 → 해당 카테고리만 검색/LLM 컨텍스트로 활용 가능
- **랭체인 Retriever, VectorStore, LLMChain 등에서 메타데이터 기반 필터링은 표준적이고 강력한 기능**

---

**정리:**
- DB 벡터(정형, 서비스 로직)와 ChromaDB 벡터(비정형, 문서 RAG)는 분리 유지 권장
- ChromaDB 벡터 생성 시 메타데이터(카테고리 등) 반드시 포함
- 특정 카테고리만 유사검색/LLM 컨텍스트로 활용하는 것도 랭체인에서 쉽게 구현 가능 

## 한글 검색 품질 및 LangChain 벡터 검색엔진 이점 Q&A (2024-06-05)

### 1. sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 vs LangChain 기반 벡터 검색엔진

- MiniLM-L12-v2는 다국어(한글 포함) 지원 임베딩 모델로, 한글도 기본 수준의 의미 임베딩 가능
- 한글 특화 모델이 아니므로, 복잡한 한글 문장/신조어/띄어쓰기 오류 등에는 의미 구분력이 다소 떨어질 수 있음
- 최신 LLM 기반 임베딩(OpenAI, Cohere, Upstage 등)에 비해 의미적 정밀도, 문맥 파악력은 낮음

### 2. LangChain 기반 벡터 검색엔진의 한글 검색 이점

- LangChain은 다양한 임베딩 모델 + 벡터DB + 검색 파이프라인을 조합하는 프레임워크
- OpenAI, Cohere, Upstage Solar 등 최신 한글 특화 임베딩 모델을 쉽게 교체/실험 가능
- 여러 임베딩 모델을 조합(하이브리드/멀티 임베딩)하여 검색 품질 극대화 가능
- 카테고리, 태그 등 메타데이터 기반 고급 검색 및 필터링 가능
- 검색 결과를 LLM(한글 지원)과 연동해 자연스러운 한글 요약, 정렬, 후처리 등 고도화 가능

### 3. 실제 한글 검색 품질 비교

| 구분 | MiniLM-L12-v2 단독 | LangChain 기반(최신 임베딩+검색엔진) |
|------|--------------------|--------------------------------------|
| 한글 의미 파악 | 보통(일상/간단 문장) | 매우 우수(최신 임베딩 활용 시) |
| 도메인 특화/신조어 | 약함 | 특화 모델/파인튜닝 가능 |
| 검색 품질 확장성 | 제한적 | 매우 유연(모델/DB/체인 교체) |
| 메타데이터 활용 | 불가 | 가능(카테고리, 태그 등) |
| 후처리/요약 | 불가 | 가능(LLM 연동) |

**정리:**
- LangChain 기반 벡터 검색엔진은 임베딩 모델 선택의 자유, 메타데이터 활용, 체인/후처리 등으로 한글 검색 품질을 MiniLM-L12-v2 단독 사용 대비 훨씬 더 높일 수 있음
- 최신 한글 특화 임베딩(OpenAI, Upstage, Cohere 등)과 결합하면 정확도, 다양성, 확장성 모두 압도적 